---
layout: post
title: jieba分词
date: 2020-04-26
tags: TATM
---
## jieba

这篇文章呢来介绍最最强大的分词工具，在此吐槽一下，我在我的mac上安装了window10的虚拟机用了30g的空间（而且耗电量惊人）就为了跑讲老师那个不到1MB的CPP分词工具。

下面我们开始吧。

我是用的是python3，所以一下都是在python 3.8的基础上实现的。平台 macbook pro 2018.

#### 安装

​	pip3 install jeiba /（windows）pip install jieba

#### 特点

​	它支持四种分词模式：

 + 精确模式：试图将橘子最精确的切开，适合文本分析
 + 全模式：把句子所有的可以成词的词语都扫描出来，速度非常快，但是不能解决歧义问题。
 + 搜索引擎模式，在精确模式的基础上，对长词在此切分，提高召回率，适合用于搜索引擎分词。
 + paddle模式，利用paddle深度学习框架，（这个东西目前搞不懂，留待以后深入解释）

#### 算法：

	+ 基于前缀词典实现高效的词图扫描，生成句子中汉子所有可能成词情况所构成的有向无环图
	+ 采用动态规划查找最大概率路径，找出基于词频的最大切分组合
	+ 对于未登录词，采用基于汉子成词能力的HMM模型（隐马尔科夫模型），使用viterbi算法。

#### 主要function：

##### 1.分词（使用UTF-8的格式）

+ jeiba.cut 方法接受四个输入参数：
  + 需要分词对的字符字符串；
  + cut_all参数用来控制是否采用全模式；
  + HMM参数用来控制是否使用HMM模型；
  + use_paddle参数用来控制是否使用paddle模式下的分词模式，

+ jieba.cut_for_search 该方法接受两个参数：需要分词的字符串；是否使用HMM模型。这个是为了搜索用的分词方法。
+ 上述两种方法生成的是一个可以迭代的generator，类似于csv中的reader
+ 同时提供了对应的jieba.lcut和jieba.lcut_for_search直接返回list;

##### 2.载入词典

+ 开发者可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率
+ 用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径
+ 词典格式和 `dict.txt` 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒。`file_name` 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码







参考：https://github.com/fxsjy/jieba

