1.简要说明单个感知机的工作原理

感知机是二分类的线性模型，其输入是实例的特征向量，输出的是事例的类别，分别是+1和-1，属于判别模型。

假设训练数据集是线性可分的，感知机学习的目标是求得一个能够将训练数据集**正实例点和负实例点完全正确分开的分离超平面**。如果是非线性可分的数据，则最后无法获得超平面。

个人理解认为感知机工作原理是先将一系列的数据通过处理，赋予一系列的权重和偏差值（刚开始都是随机的），通过不断测试（运用我们所给出的结果来测试）不断地惩罚和奖励，机器不断调整权重和偏差值，不断接近我们所给的结果，最终当机器处理结果在我们所给的允许错误范围之中时，模型建立成功，最终会将不同种类的东西加以区分（回归和分类问题感觉都有运用）。



2.简要说明线性支持向量机采用最大间距超平面的原理。

***\*先说名字\****，线性可分，就是存在一个超平面，能把正例跟负例完全分隔开，那么这个数据集就是线性可分的。

支持向量：离超平面越近的越难分是正例还是负例，要想加新的点以后预测更准确，就要使得 样本点到超平面的最小距离l最大  （距离越大越容易分类）而这个能得到最小距离l的样本点就叫做支持向量。支持向量一定是正负例都有的，它是平行于超平面，到超平面的距离为l的两个超平面上的样本点。

硬间隔：过支持向量的两个超平面之间形成了一个间隔。这两个超平面叫间隔边界。

与软间隔对应，硬间隔是存在于数据集线性可分的情况下，而软间隔是数据集近似线性可分情况。后面还没复习到，复习到再回来补充。补充：硬间隔是把所有的样本点正确分在超平面两侧，而软间隔允许某些样本点错误分类。



3.非线性支持向量机核函数的作用。

主要思想：通过非线性变换，将输入空间对应到特征空间，使得输入空间中的分离超曲面对应到特征空间的分离超平面（svm），这样，原问题可以通过求解特征空间中的线性支持向量机完成。



4.经验风险最小化和结构风险最小化

模型f(x)关于训练数据集的平均损失成为经验风险或经验损失：经验风险是模型关于训练样本集的平均损失。
经验风险最小化（empirical risk minimization,ERM）的策略认为，经验风险最小的模型是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是求解最优化问题：当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。例如，极大似然估计（MLE）就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等于极大似然估计。



结构风险最小化（structural risk minimization, SRM）是为了防止过拟合而提出的策略。结构风险最小化等价于正则化。结构风险在经验风险的基础上加上表示模型复杂度的正则化项。在假设空间、损失函数以及训练集确定的情况下，结构风险的定义是：

![image-20211112000340513](/Users/maxuan/Library/Application Support/typora-user-images/image-20211112000340513.png)

其中，J(f)为模型的复杂度，是定义在假设空间上的泛函。模型f越复杂，复杂度J(f)就越大。也就是说，复杂度表示了对复杂模型的惩罚。结构风险小的模型往往对训练数据和未知的测试数据都有较好的预测。比如，贝叶斯估计中的最大后验概率估计（MAP）就是结构风险最小化的例子。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。



5.多层感知机函数拟合的理解。

通过不停的迭代，找到最合适的权值和偏置，使得损失最小。但是这里是多层感知器模型，那么多参数，传统方式怎么计算偏导？所以对于偏导的计算就是反向传播的任务了。



6.bp神经网络的正向传播过程。



![image-20211112004001615](/Users/maxuan/Library/Application Support/typora-user-images/image-20211112004001615.png)

正向传播过程是

**1.输入层---->隐含层：**

![image-20211112003951911](/Users/maxuan/Library/Application Support/typora-user-images/image-20211112003951911.png)

**2.隐含层---->输出层：**

![image-20211112004037296](/Users/maxuan/Library/Application Support/typora-user-images/image-20211112004037296.png)

这样前向传播的过程就结束了，我们得到输出值为[0.75136079 , 0.772928465]，与实际值[0.01 , 0.99]相差还很远，现在我们对误差进行[反向传播](https://so.csdn.net/so/search?from=pc_blog_highlight&q=反向传播)，更新权值，重新计算输出。



反向传播过程

1.计算总误差

总误差：(square error)

![img](https://img-blog.csdnimg.cn/20181218102703625)

但是有两个输出，所以分别计算o1和o2的误差，总误差为两者之和：![img](https://img-blog.csdnimg.cn/20181218102703640)![img](https://img-blog.csdnimg.cn/20181218102703654)![img](https://img-blog.csdnimg.cn/20181218102703672)

 

2.隐含层---->输出层的权值更新：

以权重参数w5为例，如果我们想知道w5对整体误差产生了多少影响，可以用整体误差对w5求偏导求出：（链式法则）![img](https://img-blog.csdnimg.cn/20181218102703690)

下面的图可以更直观的看清楚误差是怎样反向传播的：

![img](https://img-blog.csdnimg.cn/20181218102838816.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzNDc5ODgx,size_16,color_FFFFFF,t_70)

 

现在我们来分别计算每个式子的值：

计算![img](https://img-blog.csdnimg.cn/20181218102703707)：

![img](https://img-blog.csdnimg.cn/20181218102703722)

计算![img](https://img-blog.csdnimg.cn/20181218102703738)：

![img](https://img-blog.csdnimg.cn/20181218102703759)

（这一步实际上就是对sigmoid函数求导，比较简单，可以自己推导一下）

 

计算![img](https://img-blog.csdnimg.cn/20181218102703774)：

![img](https://img-blog.csdnimg.cn/20181218102703799)

最后三者相乘：

![img](https://img-blog.csdnimg.cn/20181218102703814)

这样我们就计算出整体误差E(total)对w5的偏导值。

回过头来再看看上面的公式，我们发现：

![img](https://img-blog.csdnimg.cn/20181218102703830)

为了表达方便，用![img](https://img-blog.csdnimg.cn/20181218102703847)来表示输出层的误差：

![img](https://img-blog.csdnimg.cn/20181218102703865)

因此，整体误差E(total)对w5的偏导公式可以写成：

![img](https://img-blog.csdnimg.cn/20181218102703883)

如果输出层误差计为负的话，也可以写成：

![img](https://img-blog.csdnimg.cn/20181218102703901)

最后我们来更新w5的值：

![img](https://img-blog.csdnimg.cn/20181218102703918)

（其中，![img](https://img-blog.csdnimg.cn/20181218102703936)是学习速率，这里我们取0.5）

同理，可更新w6,w7,w8:

![img](https://img-blog.csdnimg.cn/20181218102703954)

 

3.隐含层---->隐含层的权值更新：

　方法其实与上面说的差不多，但是有个地方需要变一下，在上文计算总误差对w5的偏导时，是从out(o1)---->net(o1)---->w5,但是在隐含层之间的权值更新时，是out(h1)---->net(h1)---->w1,而out(h1)会接受E(o1)和E(o2)两个地方传来的误差，所以这个地方两个都要计算。

 ![img](https://img-blog.csdnimg.cn/20181218102703973)

 

计算![img](https://img-blog.csdnimg.cn/20181218102703997)：

![img](https://img-blog.csdnimg.cn/2018121810270416)

先计算![img](https://img-blog.csdnimg.cn/2018121810270427)：

![img](https://img-blog.csdnimg.cn/2018121810270446)

![img](https://img-blog.csdnimg.cn/2018121810270475)![img](https://img-blog.csdnimg.cn/2018121810270493)![img](https://img-blog.csdnimg.cn/20181218102704109)

同理，计算出：

![img](https://img-blog.csdnimg.cn/20181218102704124)

两者相加得到总值：

![img](https://img-blog.csdnimg.cn/20181218102704142)

再计算![img](https://img-blog.csdnimg.cn/20181218102704161)：

![img](https://img-blog.csdnimg.cn/20181218102704175)

再计算![img](https://img-blog.csdnimg.cn/20181218102704191)：

![img](https://img-blog.csdnimg.cn/20181218102704212)

最后，三者相乘：

![img](https://img-blog.csdnimg.cn/20181218102704230)

 为了简化公式，用sigma(h1)表示隐含层单元h1的误差：

![img](https://img-blog.csdnimg.cn/20181218102704247)

最后，更新w1的权值：

![img](https://img-blog.csdnimg.cn/20181218102704267)

同理，额可更新w2,w3,w4的权值：

![img](https://img-blog.csdnimg.cn/20181218102704285)

这样误差反向传播法就完成了，最后我们再把更新的权值重新计算，不停地迭代，在这个例子中第一次迭代之后，总误差E(total)由0.298371109下降至0.291027924。迭代10000次后，总误差为0.000035085，输出为[0.015912196,0.984065734](原输入为[0.01,0.99]),证明效果还是不错的





8.介绍卷积神经网络的工作原理和主要工作过程



一个卷积神经网络主要由以下5层组成：

- 数据输入层/ Input layer
- 卷积计算层/ CONV layer
- ReLU激励层 / ReLU layer
- 池化层 / Pooling layer
- 全连接层 / FC layer



1. 数据输入层

该层要做的处理主要是对原始图像数据进行预处理，其中包括：

- **去均值**：把输入数据各个维度都中心化为0，如下图所示，其目的就是把样本的中心拉回到坐标系原点上。
- **归一化**：幅度归一化到同样的范围，如下所示，即减少各维度数据取值范围的差异而带来的干扰，比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。
- **PCA/白化**：用PCA降维；白化是对数据各个特征轴上的幅度归一化



2. 卷积计算层

这一层就是卷积神经网络最重要的一个层次，也是“卷积神经网络”的名字来源。
在这个卷积层，有两个关键操作：

- **局部关联**。每个神经元看做一个滤波器(filter)
- **窗口(receptive field)滑动**， filter对局部数据计算



先介绍卷积层遇到的几个名词：

- **深度/depth**（解释见下图）
- **步幅/stride** （窗口一次滑动的长度）
- **填充值/zero-padding**

很好。现在，要改变每一层的行为，有两个主要参数是我们可以调整的。选择了过滤器的尺寸以后，我们还需要选择步幅（stride）和填充（padding）。

步幅控制着过滤器围绕输入内容进行卷积计算的方式。在第一部分我们举的例子中，过滤器通过每次移动一个单元的方式对输入内容进行卷积。过滤器移动的距离就是步幅。在那个例子中，步幅被默认设置为1。步幅的设置通常要确保输出内容是一个整数而非分数。让我们看一个例子。想象一个 7 x 7 的输入图像，一个 3 x 3 过滤器（简单起见不考虑第三个维度），步幅为 1。这是一种惯常的情况。



![img](https://pic4.zhimg.com/80/v2-485011463ab17396223ce79ceba0030f_1440w.jpg)



**3. 非线性层（或激活层）**

把卷积层输出结果做非线性映射。



![img](https://pic1.zhimg.com/80/v2-4f12096f7b6fb83ce6dc96b3ecf915c8_1440w.jpg)



CNN采用的激活函数一般为ReLU(The Rectified Linear Unit/修正线性单元)，它的特点是收敛快，求梯度简单。



**4.池化层**

池化层夹在连续的卷积层中间， 用于压缩数据和参数的量，减小过拟合。
简而言之，如**果输入是图像的话，那么池化层的最主要作用就是压缩图像。**

这里再展开叙述池化层的具体作用：

1. **特征不变性**，也就是我们在图像处理中经常提到的特征的尺度不变性，池化操作就是图像的resize，平时一张狗的图像被缩小了一倍我们还能认出这是一张狗的照片，这说明这张图像中仍保留着狗最重要的特征，我们一看就能判断图像中画的是一只狗，图像压缩时去掉的信息只是一些无关紧要的信息，而留下的信息则是具有尺度不变性的特征，是最能表达图像的特征。
2. **特征降维**，我们知道一幅图像含有的信息是很大的，特征也很多，但是有些信息对于我们做图像任务时没有太多用途或者有重复，我们可以把这类冗余信息去除，把最重要的特征抽取出来，这也是池化操作的一大作用。
3. 在一定程度上**防止过拟合**，更方便优化。

池化层用的方法有Max pooling 和 average pooling，而实际用的较多的是Max pooling。这里就说一下Max pooling，其实思想非常简单。



![img](https://pic4.zhimg.com/80/v2-7b28abd70e3bc4294b2b28cc6ff348ef_1440w.jpg)



对于每个2 * 2的窗口选出最大的数作为输出矩阵的相应元素的值，比如输入矩阵第一个2 * 2窗口中最大的数是6，那么输出矩阵的第一个元素就是6，如此类推。



**5.全连接层**

两层之间所有神经元都有权重连接，通常全连接层在卷积神经网络尾部。也就是跟传统的神经网络神经元的连接方式是一样的：



![img](https://pic3.zhimg.com/80/v2-9cbccaabf38a4c5c4c8494afc3556c12_1440w.jpg)

一般CNN结构依次为
\1. INPUT
\2. [[CONV -> RELU]N -> POOL?]M
\3. [FC -> RELU]*K
\4. FC



9.随机森林算法的工作原理和工作过程





10.不平衡数据分类问题面临的问题：

对比图2和图3，我们明显的看到图3的蓝色实点和橙色实点分的很开，所以图3就算是一个简单可分的不平衡数据集，所以在模型做预测的时候，并不会像图2那样每一次都预测为C0类，所以对于可分的不平衡数据集并不会有什么太大的难点。

无论特征工程挖掘的是多么深入或者模型用的是多么的复杂，不平衡数据集的分类问题由于其自身的复杂性还是会有一个理论上限，认识到这个现实后，我们其实可以稍微释怀一些，毕竟有些问题由于其数据分布的自身缺陷，无法达到很好的分类效果的！





基于规则的专家系统的主要框架、模块、推理机器

![image-20211112011423798](/Users/maxuan/Library/Application Support/typora-user-images/image-20211112011423798.png)

![image-20211112011450747](/Users/maxuan/Library/Application Support/typora-user-images/image-20211112011450747.png)

2.专家系统强调引入人类专家的经验，能够结合机器学习和数据结构构造一个新的决策支持系统。设计一个这样的系统。



3.深度学习一般包含哪些模型，相对于一般的机器学习模型的特点是什么。



概括来说，大多数情况下，这些ML算法会处理从原始数据中提取的精确特征集。特征可能非常简单，例如图像的像素值，信号的时间值，或复杂的特征（例如文字的特征表示就会复杂一些）。大多数已知的ML算法仅在要素表示数据时才能发挥作用。 正确的功能标识是至关重要的一步，它可以紧密代表当前所有数据状态。

深度学习是机器学习方法的更广泛的系列，它试图从给定的数据中学习高级功能。 因此，它解决的问题减少了为每种数据类型（语音，图像等）制作新的特征提取器的任务。

例如，对于ML中的图像识别来说，深度学习算法将在向他们呈现图像识别任务时尝试学习诸如眼睛之间的距离，鼻子的长度，或者目前无法解释的一些特征。 他们可以使用这些信息进行分类，预测等任务。 因此，这是与以前的“浅层学习算法”（ML学习）相比迈出的重要一步，也可以称为是更“智能”的一种机制。



4.LSTM相对于RNN有哪些特点，主要解决了什么问题。

在LSTM被推出之前，RNN一直是主流的深度学习语言模型，但是其存在两个十分明显的技术缺陷，一个是梯度消失，另一个是梯度爆炸。LSTM可以解决梯度消失的问题，但是不能解决梯度爆炸的问题；



5.强化学习主要应用于什么问题，思想，工作流程，举例子。

